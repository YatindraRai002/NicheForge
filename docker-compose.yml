version: '3.8'

services:
  niche-expert:
    build: .
    ports:
      - "8501:8501"
    volumes:
      - .:/app
    environment:
      - PYTHONUNBUFFERED=1
    # If using GPU locally (Windows WSL2 with NVIDIA Container Toolkit)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]
    networks:
      - app-network

  # Optional: Run Ollama in a container too (if not using local host ollama)
  # ollama:
  #   image: ollama/ollama
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ollama_data:/root/.ollama
  #   networks:
  #     - app-network

networks:
  app-network:
    driver: bridge

# volumes:
#   ollama_data:
